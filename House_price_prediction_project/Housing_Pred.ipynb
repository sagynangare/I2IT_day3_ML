{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jCUo_uAEpj2y"
   },
   "source": [
    "let’s load the data using Pandas. Once again you should write a small function to load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s471ZUQIpj2z"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "housing = pd.read_csv(\"housing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r3NhqtQ3pj24"
   },
   "source": [
    "Let’s take a look at the top five rows using the DataFrame’s head() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "colab_type": "code",
    "id": "41at6_Oapj24",
    "outputId": "a0a5476f-3afc-42cc-bd4d-e22e31f88af5"
   },
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rlK28502pj27"
   },
   "source": [
    "The info() method is useful to get a quick description of the data, in particular the total number of rows, and each attribute’s type and number of non-null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "id": "GgVlfhERpj28",
    "outputId": "26014dd2-cd1b-4e07-8fe4-1ca2fe3570aa"
   },
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RqFVrTxVpj2-"
   },
   "source": [
    "There are 20,640 instances in the dataset, which means that it is fairly small by Machine Learning standards, but it’s perfect to get started. Notice that the total_bed rooms attribute has only 20,433 non-null values, meaning that 207 districts are miss‐\n",
    "ing this feature. We will need to take care of this later.\n",
    "\n",
    "\n",
    "All attributes are numerical, except the ocean_proximity field. Its type is object, so it could hold any kind of Python object, but since you loaded this data from a CSV file you know that it must be a text attribute. When you looked at the top five rows, you probably noticed that the values in that column were repetitive, which means that it is probably a categorical attribute. You can find out what categories exist and how many districts belong to each category by using the value_counts() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 117
    },
    "colab_type": "code",
    "id": "xYXQy6aVpj2_",
    "outputId": "71eb1ec1-5f55-45d9-9463-4691e58d5c37"
   },
   "outputs": [],
   "source": [
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1oSrb5O5pj3B"
   },
   "source": [
    "Let’s look at the other fields. The describe() method shows a summary of the numerical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "aw_4aBiWpj3B",
    "outputId": "2f415515-cdeb-4b5c-f89e-a1affefaf8ac"
   },
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pO85Gmabpj3D"
   },
   "source": [
    "The count, mean, min, and max rows are self-explanatory. Note that the null values are ignored (so, for example, count of total_bedrooms is 20,433, not 20,640). The std row shows the standard deviation (which measures how dispersed the values are).\n",
    "The 25%, 50%, and 75% rows show the corresponding percentiles: a percentile indicates the value below which a given percentage of observations in a group of observations falls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "colab_type": "code",
    "id": "itSfl_Xdpj3E",
    "outputId": "f0260ef4-5ee8-48f0-b958-05876f066c37"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "housing.hist(bins=50,figsize=(25,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6KG5v5UGpj3G"
   },
   "source": [
    "# Create a Test Set\n",
    "It may sound strange to voluntarily set aside part of the data at this stage. After all, you have only taken a quick glance at the data, and surely you should learn a whole lot more about it before you decide what algorithms to use, right? This is true, but your brain is an amazing pattern detection system, which means that it is highly prone to overfitting: if you look at the test set, you may stumble upon some seemingly interesting pattern in the test data that leads you to select a particular kind of Machine Learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fPG_6bEPpj3L"
   },
   "source": [
    "#### Using Scikit Learn\n",
    "Scikit-Learn provides a few functions to split datasets into multiple subsets in various ways. The simplest function is train_test_split, which does pretty much the same thing as the function split_train_test defined earlier, with a couple of additional features. First there is a random_state parameter that allows you to set the random generator seed as explained previously, and second you can pass it multiple datasets with an identical number of rows, and it will split them on the same indices (this is very useful, for example, if you have a separate DataFrame for labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RLDN0PVWpj3L"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1B44XldRpj3N"
   },
   "source": [
    "The following code creates an income category attribute by dividing the median income by 1.5 (to limit the number of income categories), and rounding up using ceil (to have discrete categories), and then merging all the categories greater than 5 into category 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DtrcKnMWpj3O"
   },
   "outputs": [],
   "source": [
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5)\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vgYGSlOIpj3Q"
   },
   "source": [
    "Now you are ready to do stratified sampling based on the income category. For this you can use Scikit-Learn stratify argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EveLuA_3pj3T"
   },
   "source": [
    "Let’s see if this worked as expected. You can start by looking at the income category proportions in the full housing dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 117
    },
    "colab_type": "code",
    "id": "bbTJ4rt2pj3U",
    "outputId": "421f1535-2cb8-4060-e72d-0906668d23d9"
   },
   "outputs": [],
   "source": [
    "housing[\"income_cat\"].value_counts()/len(housing)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ReIHnPXTpj3Q"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2,stratify=housing[\"income_cat\"], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oZRTnQO0pj3X"
   },
   "source": [
    "You can start by looking at the income category proportions in the training set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 117
    },
    "colab_type": "code",
    "id": "781on-0ipj3X",
    "outputId": "0c87bf70-4b46-4bd8-ed99-76a9c1d98717"
   },
   "outputs": [],
   "source": [
    "train_set['income_cat'].value_counts()/len(train_set)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "daRQJhO8pj3a"
   },
   "source": [
    "You can start by looking at the income category proportions in the testing set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 117
    },
    "colab_type": "code",
    "id": "-Kdq6pCvpj3a",
    "outputId": "3df90d9f-7a1a-4a52-e80e-0fc9d4fdf9e5"
   },
   "outputs": [],
   "source": [
    "test_set['income_cat'].value_counts()/len(test_set)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fg30louopj3c"
   },
   "source": [
    "Now you should remove the income_cat attribute so the data is back to its original state:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-u8-E-Dpj3d"
   },
   "outputs": [],
   "source": [
    "housing.drop(\"income_cat\",axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "id": "kn4X5W_Cpj3f",
    "outputId": "0b7158a9-0627-42ff-928c-caf23b43b255"
   },
   "outputs": [],
   "source": [
    "train_set.drop(\"income_cat\",axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "isoenYlkpj3i"
   },
   "outputs": [],
   "source": [
    "test_set.drop(\"income_cat\",axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qijB1owupj3k"
   },
   "source": [
    "We spent quite a bit of time on test set generation for a good reason: this is an often neglected but critical part of a Machine Learning project. Moreover, many of these ideas will be useful later when we discuss cross-validation. Now it’s time to move on\n",
    "to the next stage: exploring the data\n",
    "\n",
    "\n",
    "## Discover and Visualize the Data to Gain Insights\n",
    " Let’s create a copy so you can play with it without harming the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9y1r-BJypj3l"
   },
   "outputs": [],
   "source": [
    "housing = train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cgyRYQF7pj3n"
   },
   "source": [
    "### Visualizing Geographical Data\n",
    "Since there is geographical information (latitude and longitude), it is a good idea to create a scatterplot of all districts to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "Ik96nfBPpj3n",
    "outputId": "210a6237-f668-4818-b41c-175db4ad12b7"
   },
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7yg-VWEKpj3p"
   },
   "source": [
    "Setting the alpha option to 0.1 makes it much easier to visualize the places where there is a high density of data points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "EXTNyYACpj3q",
    "outputId": "f3e9cefe-5806-49c0-bc5d-533d43fe7727"
   },
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NuNwAHSkpj3s",
    "outputId": "e6d43d21-3c74-4573-c981-aef2e760dfd3"
   },
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
    " c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
    " s=housing[\"population\"]/100,label='population',\n",
    " c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    ")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WmxNf05zpj3u"
   },
   "source": [
    "## Looking for Correlations\n",
    "Since the dataset is not too large, you can easily compute the standard correlation coefficient (also called Pearson’s r) between every pair of attributes using the corr() method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KsukC1tbpj3v"
   },
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xzTrwkJ3pj3x"
   },
   "source": [
    "Now let’s look at how much each attribute correlates with the median house value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v1nIC4zbpj3x",
    "outputId": "9f0867ac-2d10-4cdb-8601-59a4dac511bf"
   },
   "outputs": [],
   "source": [
    " corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lt65n4JXpj3z"
   },
   "source": [
    "let’s just focus on a few promising attributes that seem most correlated with the median housing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LElFBbywpj30",
    "outputId": "61239da1-fb37-48ec-a1c5-e2130bf57a88"
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iwpxHGmspj32"
   },
   "source": [
    "The most promising attribute to predict the median house value is the median income, so let’s zoom in on their correlation scatterplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qKKb0LCXpj34"
   },
   "source": [
    "## Experimenting with Attribute Combinations\n",
    "One last thing you may want to do before actually preparing the data for Machine Learning algorithms is to try out various attribute combinations. For example, the total number of rooms in a district is not very useful if you don’t know how many\n",
    "households there are. What you really want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful: you probably want to compare it to the number of rooms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3kjjdY4Jpj34"
   },
   "outputs": [],
   "source": [
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "krGxqpPJpj36",
    "outputId": "2b49b7cf-4a80-47de-e85b-8fa6e20a0140"
   },
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJK-WK5Epj38"
   },
   "source": [
    "Hey, not bad! The new bedrooms_per_room attribute is much more correlated with the median house value than the total number of rooms or bedrooms. Apparently houses with a lower bedroom/room ratio tend to be more expensive. The number of rooms per household is also more informative than the total number of rooms in a district—obviously the larger the houses, the more expensive they are.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tEO-Lxx8pj38"
   },
   "source": [
    "## Prepare the Data for Machine Learning Algorithms\n",
    "Now let's Seperated labels with the features in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HyguTAJepj39"
   },
   "outputs": [],
   "source": [
    "housing = train_set.drop(\"median_house_value\",axis=1)\n",
    "housing_labels = train_set[\"median_house_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U5c1s8LOH7VZ",
    "outputId": "019bcd86-2c09-4079-b28c-9da5517d9247"
   },
   "outputs": [],
   "source": [
    "housing = train_set.drop(\"median_house_value\",axis=1)\n",
    "\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-tkwPdBIpj3_"
   },
   "source": [
    "## Data Cleaning\n",
    "Most Machine Learning algorithms cannot work with missing features, so let’s create a few functions to take care of them. You noticed earlier that the total_bedrooms\n",
    "attribute has some missing values, so let’s fix this. You have three options:<br>\n",
    "• Get rid of the corresponding districts.<br>\n",
    "• Get rid of the whole attribute.<br>\n",
    "• Set the values to some value (zero, the mean, the median, etc.).<br>\n",
    "You can accomplish these easily using DataFrame’s dropna(), drop(), and fillna() methods:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NN0W9ydZpj4C"
   },
   "source": [
    "Scikit-Learn provides a handy class to take care of missing values: Imputer. Here is\n",
    "how to use it. First, you need to create an Imputer instance, specifying that you want\n",
    "to replace each attribute’s missing values with the median of that attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7zhcfeMhpj4D"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9gOPcWoupj4E"
   },
   "source": [
    "Since the median can only be computed on numerical attributes, we need to create a\n",
    "copy of the data without the text attribute ocean_proximity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kW11rP32pj4F"
   },
   "outputs": [],
   "source": [
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "krBdwsIWpj4G"
   },
   "source": [
    "Now you can fit the imputer instance to the training data using the fit() method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "HjkPD140pj4H",
    "outputId": "a83c1c18-9eeb-4548-9780-49c610d63835"
   },
   "outputs": [],
   "source": [
    "imputer.fit(housing_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5SWaxNzHpj4J"
   },
   "source": [
    "The imputer has simply computed the median of each attribute and stored the result\n",
    "in its statistics_ instance variable. Only the total_bedrooms attribute had missing\n",
    "values, but we cannot be sure that there won’t be any missing values in new data after\n",
    "the system goes live, so it is safer to apply the imputer to all the numerical attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "E9OLO9G5pj4K",
    "outputId": "ccb822c0-d060-440f-ee04-59f1f194b10c"
   },
   "outputs": [],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "ebdFuoSJpj4M",
    "outputId": "2fbbfc4e-c944-434e-cec6-dc09e554eba9"
   },
   "outputs": [],
   "source": [
    " housing_num.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GMpWGUktpj4P"
   },
   "source": [
    "Now you can use this “trained” imputer to transform the training set by replacing\n",
    "missing values by the learned medians:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktN1T-1apj4P"
   },
   "outputs": [],
   "source": [
    "X = imputer.transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5lkSp48TH7V0",
    "outputId": "f26dacdd-292c-4c19-94b2-c0c056e07f16"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8pDfvKEhpj4R"
   },
   "source": [
    "The result is a plain Numpy array containing the transformed features. If you want to\n",
    "put it back into a Pandas DataFrame, it’s simple:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z-zIPGBapj4S"
   },
   "outputs": [],
   "source": [
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a48GrpgXpj4W"
   },
   "source": [
    "## Handling Text and Categorical Attributes\n",
    "Earlier we left out the categorical attribute ocean_proximity because it is a text\n",
    "attribute so we cannot compute its median. Most Machine Learning algorithms pre‐\n",
    "fer to work with numbers anyway, so let’s convert these text labels to numbers.\n",
    "Scikit-Learn provides a transformer for this task called LabelEncoder:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9TkMmq0Upj4W",
    "outputId": "f5f90a60-b4bb-4893-f181-8fdc2a4ee53f"
   },
   "outputs": [],
   "source": [
    "housing_cat = housing[\"ocean_proximity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QF2eJ9LNpj4X"
   },
   "source": [
    "This is better: now we can use this numerical data in any ML algorithm. You can look\n",
    "at the mapping that this encoder has learned using the classes_ attribute (“<1H\n",
    "OCEAN” is mapped to 0, “INLAND” is mapped to 1, etc.):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0My94uXBpj4Z"
   },
   "source": [
    "Scikit-Learn provides a OneHotEncoder encoder to convert integer categorical values\n",
    "into one-hot vectors. Let’s encode the categories as one-hot vectors. Note that\n",
    "fit_transform() expects a 2D array, but housing_cat_encoded is a 1D array, so we\n",
    "need to reshape it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6jU6QaMapj4c"
   },
   "source": [
    "Using up tons of memory mostly to store zeros would\n",
    "be very wasteful, so instead a sparse matrix only stores the location of the nonzero\n",
    "elements. You can use it mostly like a normal 2D array,19 but if you really want to con‐\n",
    "vert it to a (dense) NumPy array, just call the toarray() method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pO-HsEKQpj4d"
   },
   "source": [
    "We can apply both transformations (from text categories to integer categories, then\n",
    "from integer categories to one-hot vectors) in one shot using the LabelBinarizer\n",
    "class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ud8Q7XoTpj4e"
   },
   "source": [
    "We can apply both transformations (from text categories to integer categories, then\n",
    "from integer categories to one-hot vectors) in one shot using the LabelBinarizer\n",
    "class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z_wGCeuepj4e",
    "outputId": "3dca643a-59cd-4f2a-ec9f-f25170b9777f"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "housing_cat_1hot = lb.fit_transform(housing_cat)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dasAvH_Rpj4k"
   },
   "source": [
    "## Transformation Pipelines\n",
    "As you can see, there are many data transformation steps that need to be executed in\n",
    "the right order. Fortunately, Scikit-Learn provides the Pipeline class to help with\n",
    "such sequences of transformations. Here is a small pipeline for the numerical\n",
    "attributes:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XlG7Hq94pj4k"
   },
   "source": [
    "You now have a pipeline for numerical values, and you also need to apply the LabelBi\n",
    "narizer on the categorical values: how can you join these transformations into a sin‐\n",
    "gle pipeline? Scikit-Learn provides a FeatureUnion class for this. You give it a list of\n",
    "transformers (which can be entire transformer pipelines), and when its transform()\n",
    "method is called it runs each transformer’s transform() method in parallel, waits for\n",
    "their output, and then concatenates them and returns the result (and of course calling\n",
    "its fit() method calls all each transformer’s fit() method). A full pipeline handling\n",
    "both numerical and categorical attributes may look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VBy3pY9zpj4k"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mJpxoL3lpj4m"
   },
   "outputs": [],
   "source": [
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6YRNgDHqpj4o"
   },
   "source": [
    "Each subpipeline starts with a selector transformer: it simply transforms the data by\n",
    "selecting the desired attributes (numerical or categorical), dropping the rest, and con‐\n",
    "verting the resulting DataFrame to a NumPy array. There is nothing in Scikit-Learn\n",
    "to handle Pandas DataFrames, so we need to write a simple custom transformer for\n",
    "this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jf0ZN27Zpj4o"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oNBVjty6pj4q"
   },
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    " ('selector', DataFrameSelector(num_attribs)),\n",
    " ('imputer', SimpleImputer(strategy=\"median\")),\n",
    " ])\n",
    "cat_pipeline = Pipeline([\n",
    " ('selector', DataFrameSelector(cat_attribs)),\n",
    " ('one_hot_encoder', OneHotEncoder(sparse=False)),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    " (\"num_pipeline\", num_pipeline),\n",
    " (\"cat_pipeline\", cat_pipeline),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4QBsdQnWpj4r",
    "outputId": "88188536-95d1-4069-ca89-9ed520c020dd"
   },
   "outputs": [],
   "source": [
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "print(housing_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y4YB9K7bpj4t"
   },
   "source": [
    "## Select and Train a Model\n",
    "At last! You framed the problem, you got the data and explored it, you sampled a\n",
    "training set and a test set, and you wrote transformation pipelines to clean up and\n",
    "prepare your data for Machine Learning algorithms automatically. You are now ready\n",
    "to select and train a Machine Learning model.\n",
    "## Training and Evaluating on the Training Set\n",
    "The good news is that thanks to all these previous steps, things are now going to be\n",
    "much simpler than you might think. Let’s first train a Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TTEB-aPnpj4u",
    "outputId": "aa781db5-80b2-46e9-ca34-51ffa945a950"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "woRi-X4xpj4w",
    "outputId": "a0e1a845-27a5-42e0-b2f4-f9ada7cd2686"
   },
   "outputs": [],
   "source": [
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "print(\"Predictions:\\t\", lin_reg.predict(some_data_prepared).astype(\"int64\"))\n",
    "print(\"Labels:\\t\\t\", list(some_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hZ4jkN9Tpj4x"
   },
   "source": [
    "It works, although the predictions are not exactly accurate (e.g., the second prediction\n",
    "is off by more than 50%!). Let’s measure this regression model’s RMSE on the whole\n",
    "training set using Scikit-Learn’s mean_squared_error function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jUwZjRKRpj4y",
    "outputId": "cdccccc5-3562-49ca-d746-cdc64947d1bd"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YXgzzEwapj4z"
   },
   "source": [
    "Let’s train a DecisionTreeRegressor. This is a powerful model, capable of finding\n",
    "complex nonlinear relationships in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nvH2f9Wypj4z",
    "outputId": "10852740-1cd4-4f82-e1b2-f83e39f556cd"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZPAc4Bvypj41",
    "outputId": "6d3e1040-6a7c-4bd7-a295-5fb299d8cf56"
   },
   "outputs": [],
   "source": [
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "13RInw6kpj44"
   },
   "source": [
    "Wait, what!? No error at all? Could this model really be absolutely perfect? Of course,\n",
    "it is much more likely that the model has badly overfit the data. How can you be sure?\n",
    "As we saw earlier, you don’t want to touch the test set until you are ready to launch a\n",
    "model you are confident about, so you need to use part of the training set for train‐\n",
    "ing, and part for model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vZZOLQChpj44"
   },
   "source": [
    "## Better Evaluation Using Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fqkHgYZLpj44"
   },
   "source": [
    "A great alternative is to use Scikit-Learn’s cross-validation feature. The following code\n",
    "performs K-fold cross-validation: it randomly splits the training set into 10 distinct\n",
    "subsets called folds, then it trains and evaluates the Decision Tree model 10 times,\n",
    "picking a different fold for evaluation every time and training on the other 9 folds.\n",
    "The result is an array containing the 10 evaluation scores:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TtYVTb2Rpj44"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,scoring=\"neg_mean_squared_error\", cv=10)\n",
    "rmse_scores = np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scores(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EL03yWUepj48"
   },
   "source": [
    "Now the Decision Tree doesn’t look as good as it did earlier. In fact, it seems to per‐\n",
    "form worse than the Linear Regression model! Notice that cross-validation allows\n",
    "you to get not only an estimate of the performance of your model, but also a measure\n",
    "of how precise this estimate is (i.e., its standard deviation). The Decision Tree has a\n",
    "score of approximately 71,200, generally ±3,200. You would not have this information\n",
    "if you just used one validation set. But cross-validation comes at the cost of training\n",
    "the model several times, so it is not always possible.\n",
    "Let’s compute the same scores for the Linear Regression model just to be sure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GDBkvGuEpj48",
    "outputId": "dd4b9b86-c43a-41c3-a7d3-703b8c0b56a2"
   },
   "outputs": [],
   "source": [
    "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,scoring=\"neg_mean_squared_error\", cv=10)\n",
    "rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LgTryUWvpj49"
   },
   "source": [
    "That’s right: the Decision Tree model is overfitting so badly that it performs worse\n",
    "than the Linear Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OJ3NS_0vpj4-"
   },
   "source": [
    "Let’s try one last model now: the RandomForestRegressor. Random Forests work by training many Decision Trees on random subsets of\n",
    "the features, then averaging out their predictions. Building a model on top of many\n",
    "other models is called Ensemble Learning, and it is often a great way to push ML algo‐\n",
    "rithms even further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VwN2jP1Apj4-",
    "outputId": "5a05f08c-c120-46ed-c172-1dcbacac7d20"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "forest_rmse = cross_val_score(forest_reg, housing_prepared, housing_labels,scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_rmse)\n",
    "print(forest_rmse)\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dpTwSwmWpj5B"
   },
   "source": [
    "Wow, this is much better: Random Forests look very promising. However, note that\n",
    "the score on the training set is still much lower than on the validation sets, meaning\n",
    "that the model is still overfitting the training set. Possible solutions for overfitting are\n",
    "to simplify the model, constrain it (i.e., regularize it), or get a lot more training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zU0Gy9i6pj5B"
   },
   "source": [
    "You can easily save\n",
    "Scikit-Learn models by using Python’s pickle module, or using\n",
    "joblib, which is more efficient at serializing\n",
    "large NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-_7hvz_pj5B",
    "outputId": "6cddeaa8-32ec-4810-c453-23cee75cf99a"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(forest_reg, \"RandomForest1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j_4x8sPwpj5C"
   },
   "source": [
    "## Fine-Tune Your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lT_TcMB2pj5D"
   },
   "source": [
    "## Grid Search\n",
    "One way to do that would be to fiddle with the hyperparameters manually, until you\n",
    "find a great combination of hyperparameter values. This would be very tedious work,\n",
    "and you may not have time to explore many combinations.\n",
    "Instead you should get Scikit-Learn’s GridSearchCV to search for you. All you need to\n",
    "do is tell it which hyperparameters you want it to experiment with, and what values to\n",
    "try out, and it will evaluate all the possible combinations of hyperparameter values,\n",
    "using cross-validation. For example, the following code searches for the best combi‐\n",
    "nation of hyperparameter values for the RandomForestRegressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cn72nJehpj5D",
    "outputId": "6fbbec57-68b6-4e4d-afc3-14032220c0bc"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    " {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    " {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    " ]\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,scoring='neg_mean_squared_error')\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYxEK9V6pj5E"
   },
   "source": [
    "It may take quite a long time, but when it is done you can get the\n",
    "best combination of parameters like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tTwq7riipj5F",
    "outputId": "1bac6e74-952c-4987-a1c8-b8f70d329cb8"
   },
   "outputs": [],
   "source": [
    "grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gAXlhIsgpj5G"
   },
   "source": [
    "You can also get the best estimator directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kAWEwphhpj5G",
    "outputId": "f14dc58c-d58b-4883-9357-c7777f45d357"
   },
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jJSMqnWppj5H"
   },
   "source": [
    "And of course the evaluation scores are also available:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KNZqZNmFpj5H",
    "outputId": "ccca44f5-09e7-448b-e25d-4226a25d3d2b"
   },
   "outputs": [],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fvH8mE1cpj5I"
   },
   "source": [
    "In this example, we obtain the best solution by setting the max_features hyperparameter to 6, and the n_estimators hyperparameter to 30. The RMSE score for this\n",
    "combination is 49,959, which is slightly better than the score you got earlier using the\n",
    "default hyperparameter values (which was 52,634). Congratulations, you have successfully fine-tuned your best model!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lmSuVFEppj5M"
   },
   "source": [
    "## Evaluate Your System on the Test Set\n",
    "After tweaking your models for a while, you eventually have a system that performs\n",
    "sufficiently well. Now is the time to evaluate the final model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zTatlFgqpj5M"
   },
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "X_test = test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = test_set[\"median_house_value\"].copy()\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7C0-u5KUpj5P",
    "outputId": "6421ad19-1f11-43dd-ba0e-d5d8e567268d"
   },
   "outputs": [],
   "source": [
    "final_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8kOKm20Tpj5Q",
    "outputId": "5bd77243-a83a-4ea8-b098-4804c18444c0"
   },
   "outputs": [],
   "source": [
    "final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ib0mCJcLH7W4",
    "outputId": "b831854f-3725-4da4-9524-a8701f8c9800"
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q7q4gtQ3H7W7",
    "outputId": "e94ea27d-ff03-48da-ab43-24dd865ff8e4"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor(n_estimators=280,max_features=6)\n",
    "forest_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q2XBwg8DH7W8"
   },
   "outputs": [],
   "source": [
    "fp=forest_reg.predict(X_test_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AU1d_WteH7W9",
    "outputId": "f92b0ef8-a8a1-487f-d622-8a52daeb359e"
   },
   "outputs": [],
   "source": [
    "fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y2pAFNMhH7W_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "fPG_6bEPpj3L",
    "qijB1owupj3k",
    "cgyRYQF7pj3n",
    "WmxNf05zpj3u",
    "qKKb0LCXpj34",
    "tEO-Lxx8pj38",
    "-tkwPdBIpj3_",
    "a48GrpgXpj4W",
    "sCzP7TRepj4g",
    "dasAvH_Rpj4k",
    "Y4YB9K7bpj4t",
    "vZZOLQChpj44",
    "j_4x8sPwpj5C",
    "lT_TcMB2pj5D",
    "B39uQxBxpj5J",
    "lmSuVFEppj5M"
   ],
   "name": "Housing (1).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
